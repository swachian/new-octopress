<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[分类: linux | Hegel2011的博客]]></title>
  <link href="http://octopresszhangyu.herokuapp.com/blog/categories/linux/atom.xml" rel="self"/>
  <link href="http://octopresszhangyu.herokuapp.com/"/>
  <updated>2022-01-01T21:15:50+08:00</updated>
  <id>http://octopresszhangyu.herokuapp.com/</id>
  <author>
    <name><![CDATA[Hegel 2011]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[The Kubernetes Book]]></title>
    <link href="http://octopresszhangyu.herokuapp.com/blog/2021/10/26/the-kubernetes-book/"/>
    <updated>2021-10-26T09:47:00+08:00</updated>
    <id>http://octopresszhangyu.herokuapp.com/blog/2021/10/26/the-kubernetes-book</id>
    <content type="html"><![CDATA[<p>The Kubernetes book, 购于Leanpub</p>

<h1>1&amp;2 Kubernetes 基础和操作</h1>

<h2>Control plane node (Masters, Heads)</h2>

<p>The API Server<br/>
   所有的入口</p>

<p>The cluster store(etcd)<br/>
    分布式数据库，3-5个拷贝，采用RAFT算法，由一个领袖说了算
    C优先于A，一旦不一致，就不能再更新了</p>

<p>The Controller manager<br/>
    Deployment, StatefulSet, ReplicaSet</p>

<p>The Scheduler<br/>
    检查Node的状况，给Node评分，以挑选合适的可以执行任务的Node</p>

<p>The cloud controller manager<br/>
    提供公有云服务的load-balancer到app的连接等</p>

<h2>Worker(Kubelet)</h2>

<p>Kubelet<br/>
    安装在主机上的代理，负责向heads汇报状态，并接受任务安排</p>

<p>Containerd<br/>
    从Docker抽象出来的接口，K8S已经不再直接支持Docker</p>

<p>kube-proxy<br/>
    运行在node上的网络组件，给每个node分配不同的docker内网，建立iptables表格完成路由功能</p>

<p>Kubernetes DNS<br/>
    每个Pod都有一个静态的Ip地址写了DNS服务，基于开源CoreDNS项目开发</p>

<h2>声明模式和渴望的状态</h2>

<ol>
<li>在manifest文件里声明目标状态（image，replicas数量，网络端口，如何更新）</li>
<li>Post给API服务（kubectl命令）</li>
<li>Kubernetes存入集群store（解析并分解成操作步骤入库）</li>
<li>Kubernetes实现目标状态(安排任务执行)</li>
<li>一个controller确保观察到的状态不会和目标状态不同(reconcile循环)</li>
</ol>


<h2>Pods</h2>

<p>A flock of birds 一群鸟 A pod of whales 一群鲸<br/>
所以pod就是一群dockers/containers</p>

<h2>Deployments</h2>

<p>一般不会直接部署pods，而是通过Controllers进行部署</p>

<h2>Service</h2>

<p>提供TCP UDP层面稳定的网络抽象</p>

<h1>3. 部署Kubernetes</h1>

<p>AWS: EKS, Azure: AKS, Google: GKE<br/>
腾讯: TKE, 阿里云: ACS（C是container）的缩写</p>

<p>可以在自己的机器上部署K8S作为练习：如docker Desktop，k3d, 开发可以用单机版的尝试</p>

<h2>kubectl</h2>

<p>~/.kube/config，中存放了命令行的操作配置</p>

<p>Clusters，Users和Contexts，前二者可以通过最后一个进行关联，表示用哪个用户的账号去操作哪个cluster</p>

<p><code>kubectl config current-context</code>用于查看当前活跃的context<br/>
<code>kubectl config use-context docker2</code> 用于调整当前活跃的context</p>

<h1>4. 用Pods工作</h1>

<ol>
<li>让资源共享：文件系统、同一个IP、共享内存、共享存储</li>
<li>一个pod中可以有多个containers，同一pod内的container可以通过localhost+不同的端口进行通信</li>
<li>通过Controller部署的pods才有高可用等特性支持</li>
<li>直接通过Pod manifest部署的pod，只有自生自灭</li>
</ol>


<p>Pod本身就是容器</p>

<h2>The pod network</h2>

<p>Pod的网络叫做pod network，是一种扁平的、各个pod可以彼此直接访问的组网。</p>

<h2>生命周期有长有短的pod</h2>

<p>long-lived: Deployments, StatefulSets, DaemonSets<br/>
short-lived: CronJobs, Jobs</p>

<p>长的多用于服务，短的多用于批量job。</p>

<h2>多Containers的几种花样</h2>

<p>Sidecar: 德国摩托车旁边的侧车；即一个container为主，一个为辅的模式。serice mesh里面常常用来对流量加密、暴露测量的数据<br/>
Adapter: sidecar的变种，比如把nginx的日志转变成普罗米修斯能解析的格式<br/>
Ambassador：也是变种，含义上讲更像是主container的使者<br/>
Init: 在拉起主app之前，执行一些辅助工作，通常只执行一次</p>

<p><code>kubectl apply -f pod.yml</code> 运行一个简单的pod，下面两个命令用于查询pod及其内容器的状态<br/>
<code>kubectl describe pods hello-pod</code><br/>
<code>kubectl get pods  hello-pod  -o wide</code></p>

<p>普通的执行命令 <code>kubectl exec hello-pod -- ps aux</code><br/>
获取pod内shell的命令 <code>kubectl exec -it hello-pod -- sh</code>, 注意需要加上<code>it</code></p>

<h1>5. Namespace</h1>

<p>查看命名空间 <code>kubectl get namespaces</code>  可以缩写为<code>ns</code></p>

<p>用kubectl可以命令式创建ns，用yaml文档可以声明式创建<br/>
<code>kubectl create ns hydra</code><br/>
<code>kubectl apply -f shields-ns.yml</code></p>

<p>切换默认的ns <code>kubectl config set-context --current --namespace ss</code></p>

<p>声明式的清除 <code>kubectl delete -f shield-app.yml</code></p>

<h1>6. Kubernetes部署</h1>

<p>Workload apis: Deployments, DaemonSets, StatefulSets</p>

<p>Deployments是aggregation root，内部其实还有ReplicaSet， 通过ReplicaSet来操作Pods</p>

<p>Reconcile：补锅，补偿，使得现实变成目标</p>

<pre><code class="yaml">piVersion: apps/v1
kind: Deployment
metadata:
  name: hello-deploy
spec:
  replicas: 10
  selector:
    matchLabels:
      app: hello-world
  minReadySeconds: 10
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
  template:
    metadata:
      labels:
        app: hello-world
    spec:
      containers:
      - name: hello-pod
        image: nigelpoulton/k8sbook:latest
        ports:
        - containerPort: 8080
</code></pre>

<p><code>.spec.selector</code>中的label选择了template中的app，注意label Selectors</p>

<p><code>ubectl get deploy hello-deploy</code><br/>
<code>kubectl scale deploy hello-deploy --replicas 5</code></p>

<p>rollback， clean-up</p>

<h1>7. Service</h1>

<ol>
<li>Service是一个K8S对象</li>
<li>Service有稳定的ip地址(a cluster ip)、dns名字以及port口</li>
<li>利用labels和selectors去动态地选择pods以传递流量</li>
</ol>


<p>从外部访问的方法：1. NodePort， 2. LoadBalancer</p>

<p>使用DNS服务作为服务发现的核心。Service名称会自动注册到DNS集群中，每个pod及其container都预先配置了dns集群的配置，以解决Service Name 到 ClusterIP之间的映射</p>

<h1>8. Ingress</h1>

<p>Ingress属于七层交换的概念，可以通过域名或路径等组合来转发请求</p>

<p>NodePorts的缺点： 使用的port偏大，30000-32767<br/>
LoadBalancer缺点： 需要和云的load-balancer 1-to-1 对应，开销容易很大<br/>
Ingress可以在只有一个云load-balancer的情况下完成多个服务的路由</p>

<p>不过，如果使用一个service mesh，可能Ingress的作用就不大了。</p>

<p>Ingress也是一种controller，只是大部分不是内建的。</p>

<p>可以使用helm创建nginx-ingress</p>

<pre><code>helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
helm repo update
helm install ingress-nginx ingress-nginx/ingress-nginx --namespace ss
kubectl get pods -n ss -o wide

ingress-nginx-controller-5c8d66c76d-2kq2f   1/1     Running   0          49m   10.244.2.34   master-node   &lt;none&gt;           &lt;none&gt;

kubectl get svc
NAME                                 TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
ingress-nginx-controller             LoadBalancer   10.108.112.4    &lt;pending&gt;     80:32003/TCP,443:30330/TCP   50m
</code></pre>

<p>但因为是自建的kubeadm，所以没有云原生的直接load-balance的支持，但可以通过32003端口对服务进行访问。</p>

<h1>9. 服务发现详解</h1>

<pre><code>kubectl get pods -n kube-system  -o wide
NAME                                           READY   STATUS    RESTARTS      AGE   IP             NODE                   NOMINATED NODE   READINESS GATES
coredns-78fcd69978-kb5td                       1/1     Running   1 (8d ago)    59d   10.244.0.4     office-wallet-dev-17   &lt;none&gt;           &lt;none&gt;
coredns-78fcd69978-l6m5g                       1/1     Running   1 (8d ago)    59d   10.244.0.5     office-wallet-dev-17   &lt;none&gt;           &lt;none&gt;
</code></pre>

<p>control plane上有dns集群</p>

<p>svc部署后会分配一个vip，也就是clusterIp；因为svc name最终会进入dns服务，所以这也是svc的命名规则要求和dns域名一致的根本原因</p>

<p>Kube-proxy会完成对Endpoints的监控和转发调整。现代版本的linux普遍使用IPVS，老的使用iptables。</p>

<p>clusterIp是一个不存在的IP，所以container传给node，node传给它的gateway（node的kernel，IPVS或Iptables），由kube-proxy负责对ClusterIp的重定向。
IPVS是L4 load-balancer，扩展的性能更优越。</p>

<p>服务发现和namespace</p>

<p><code>&lt;object-name&gt;.&lt;namespace&gt;.svc.cluster.local</code></p>

<p><code>kubectl get all -n dev</code> 可以同时列出 svc pods</p>

<h1>10. Storage</h1>

<p>Storage采用的CSI plugin的机制。</p>

<p>SC - StorageClass 定义存储的硬件<br/>
PVC - 变成PV的请求，也是Pod可以使用的存储  <br/>
PV - SC检测到PVC后，会变成PV</p>

<ul>
<li>PVC里的Access mode:

<ul>
<li>ReadWriteOnce(RWO)传统的一块硬盘</li>
<li>ReadWriteMany(RWM) NFS这种支持多个加载</li>
<li>ReadOnlyMany(ROM)  可被多少只读</li>
</ul>
</li>
</ul>


<p>PV只能按一种模式打开</p>

<p><code>kubectl get sc</code></p>

<p>解除后数据是否删除的策略：1. Delete 2. Retain<br/>
Sc中的VolumeBindingModo: 1. Immediate 2. WaitingForFirstConsumer(Pod)</p>

<pre><code class="yaml">apiVersion: v1
kind: Pod
metadata:
  name: volpod
spec:
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: pvc-prem
  containers:
  - name: ubuntu-ctr
    image: ubuntu:latest
    command:
    - /bin/bash
    - "-c"
    - "sleep 60m"
    volumeMounts:
    - mountPath: /data
      name: data
</code></pre>

<h1>11. ConfigMaps and Secrets</h1>

<p>ConfigMaps是键值对，value可以是一个简单的值，也可以是一整个配置文件。<br/>
注入container有几种方式：1. 环境变量 2. 启动的命令 3. volume中的文件<br/>
env var； cmd； vol</p>

<p><code>kubectl create configmap</code></p>

<p>vol最大的优势是一旦修改了configMap，可以在container的mount位置立刻看见修改。而两位两个必须重启容器。即Vol的注入是动态的，env的注入是静态的。</p>

<p>Secret只是以base64的形势存放起来</p>

<h1>12. StatefulSets</h1>

<p>与Deployment相比的差异：<br/>
1. 可预知的持久的pod名字<br/>
2. 可预知的持久的DNS主机名字<br/>
3. 可预知的持久度vol绑定</p>

<p>pod生成的规则：<StatefuleSetName>-<Integer>，而且是按顺序依次启动，完成前一个，才启动后一个。</p>

<p>Stateful controllers自己直接处理治愈和扩容，而不是假手ReplicaSet</p>

<h2>headless service and StatefulSet's governing service</h2>

<p>headless service就是clusterIP为空的service，搭配StatuefulSet，就会产生独立的每个pod的dns名称。app的调用也会基于具体的pod 名称。</p>

<p><code>volumeClaimTemplates</code>用于给每个stateful pod创建动态PVC。</p>

<p>独立pod的dns命名规则为<object-name>.<service-name>.<namespace>.svc.cluster.local</p>

<p>如果是pod退出，那么StatefulSet可以很容易地重新创建一个替代的pod。但如果是node不正常，则需要手工干预。</p>

<h1>13. API 安全和RBAC</h1>

<p>user,gourp, Service acct -> API server -> authN -> authZ -> Admission control</p>

<p>Kubernetes没有自己的identity database，主要通过和其他auth模块集成（plug-in）的方式来实现authN。<br/>
比如client certs，webhooks，IAM（Identity Access Management)</p>

<p>K8S的authZ只有allow没有deny，因为默认全部都是deny。</p>

<pre><code class="yaml">rules:
  - apiGroups: ["apps"]
    resources: ["deployments"]
    verbs: ["get", "watch", "list"]
</code></pre>

<p><code>*</code>意味着全部授权</p>

<p>4个RBAC对象</p>

<ul>
<li>Roles</li>
<li>ClusterRoles</li>
<li>RoleBindings</li>
<li>ClusterRoleBindings</li>
</ul>


<p>顾名思义，可以使用的namespace是不同的。需要注意的是，ClusterRoles经常和RoleBindings搭配使用。</p>

<p>Admission controller有很多个，实际主要负责批准相应的请求。</p>

<h1>14. The Kubernetes API</h1>

<p>JSON + Protobuf支持，</p>

<p>可以自己定义并扩展API</p>

<h1>15. Threat和K8S</h1>

<p>STRIDE 模型</p>

<p>Spoofing欺骗:<br/>
  - 通信层面的安全TLS<br/>
  - 限制pod和API server的通信，token的失效期等等</p>

<p>Tampering篡改:<br/>
  - 组件的篡改：etcd，api server，controller，scheduler，kubelet，镜像运行时，镜像文件等<br/>
  - 运行的pod修改</p>

<p>Repudiation否认:<br/>
  - 中央化的收集各个组件的审计日志</p>

<p>Information Disclosure 信息泄露:<br/>
  - KMS和HSM是实现key和数据在节点上分离</p>

<p>Denial of Service(DOS):<br/>
  - HA<br/>
  - WAF</p>

<p>Elevation of privilege:</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[iptables 的一些解释]]></title>
    <link href="http://octopresszhangyu.herokuapp.com/blog/2016/04/29/iptables-de-yi-xie-jie-shi/"/>
    <updated>2016-04-29T09:45:00+08:00</updated>
    <id>http://octopresszhangyu.herokuapp.com/blog/2016/04/29/iptables-de-yi-xie-jie-shi</id>
    <content type="html"><![CDATA[<p>在<strong>物理机</strong>部署的年代里，组网通常是分层而建。例如所有的服务器都接到二层交换机组成<strong>内网</strong>，随后交换机上联三层交换机及防火墙。彼时，防火墙通常由专用
的设备承担。所有的路由规则、端口开放等也均在网络设备上进行配置。对于服务器而言，通常最简单的做法就是关闭iptables，在内部实现完全互通，而把防护任务交给专用设备。</p>

<p>然而转到<strong>云部署</strong>的年代，专用的网络设备大都收归云所有，此时虽然还有内网的概念，但已经模糊，从而要求把防火墙建在主机上。做到几个域的隔离。</p>

<p>iptables的配法有命令行和配置文件，此处以配置文件<code>/etc/sysconfig/iptables</code>为例，说明一些概念。</p>

<p>对于安全类的防护,集中在<code>*filter</code>上，nat 等是管映射和路由的</p>

<pre><code>*filter
:INPUT ACCEPT [1194191:146127015]
:FORWARD ACCEPT [159938:8304428]
:OUTPUT ACCEPT [1044829:126818322]
</code></pre>

<p>上面这段代码表示对三种包的处理状态及已处理的包和字节数量。
<code>INPUT</code>就是输入的包，<code>ACCEPT</code>表示默认都可以进入，如果设置成<code>DROP</code>就是都废弃输入包了。如果主要以开白名单的方式，<code>INPUT后</code>可以接<code>DROP</code>。中括号里面的数字，冒号前是包数量统计，冒号后是字节数统计。<br/>
<code>FORWARD</code>表示转发，<code>OUTPUT</code>表示输出访问，通常这两个可以设置成<code>ACCEPT</code>即表示允许。<br/>
这3个表示的是基础的规则，类似默认路由，优先级是最低的。</p>

<pre><code>-A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT
</code></pre>

<p>随后就是很重要的这一段。这段的含义表示对连接状态是<code>ESTABLISHED</code>, <code>RELATED</code>的链接全部放行。iptables中的链接共有4种，<code>established</code>表示链接已建立，<code>related</code>的含义是由已建立的链接产生的链接，此外还有<code>new</code>和<code>invalid</code>两种链接状态，
其实这段话的含义也可以理解成<code>invalid</code>链接放弃，<code>new</code>状态之外的连接则允许通过。
这个指示关键的原因在于，比如本机设置了禁止外网访问本机的各个端口，同时本机作为客户端访问了另一个外网服务，那么外网服务的响应信息允许被接收。否则，就会面临只有一个单工链路的情况。</p>

<pre><code>-A INPUT -d 192.168.1.202 -p tcp -m tcp --dport 80 -j ACCEPT
-A INPUT -d 192.168.1.202 -p tcp -m tcp --dport 8080 -j ACCEPT
-A INPUT -d 192.168.1.202 -p tcp -m tcp --dport 8081 -j ACCEPT
-A INPUT -d 192.168.1.202 -p tcp -m tcp --dport 22 -j ACCEPT
-A INPUT -d 192.168.1.202 -p tcp -m tcp --dport 21 -j ACCEPT
-A INPUT -d 192.168.1.202 -p tcp -m tcp --dport 20 -j ACCEPT
-A INPUT -d 192.168.1.202 -s 10.3.1.0/24 -j ACCEPT
-A INPUT -d 192.168.1.202 -j DROP

-A INPUT -d 10.10.10.222 -j DROP
</code></pre>

<p>这个机器有两块网卡，分别有两个地址<code>192.168.1.202</code>和<code>10.10.10.222</code>,对于后者，只允许从这个口子发出包以及收入已建立连接的包（通过上面的 <code>-A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT</code>)。<br/>
而对于前一个地址，则打开了http相关的80 8080 8081端口，开放了ssh和ftp端口，并对来自10.3.1.0网段的全部IP开放访问权限。</p>

<p>对于包的动作有ACCEPT DROP 和REJECT，后两个都表示拒绝，只是REJECT是明确拒绝，DROP只是沉默不答。</p>

<p>对于iptables，简单的用用，安全防护做到这样也就够了。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[nscd]]></title>
    <link href="http://octopresszhangyu.herokuapp.com/blog/2015/06/19/nscd/"/>
    <updated>2015-06-19T13:56:00+08:00</updated>
    <id>http://octopresszhangyu.herokuapp.com/blog/2015/06/19/nscd</id>
    <content type="html"><![CDATA[<p>NSCD是name service cache daemon的缩写，主要用于在linux下对dns查询的结果进行缓存。
默认情况下，linux是不对dns进行缓存的，因此可能会对应用的性能产生影响。只有开启了nscd服务，才会拥有缓存。</p>

<p>服务的安装很简单，</p>

<pre><code class="sh">yum install nscd
service nscd start
</code></pre>

<p>这样就可以开启nscd的服务了。
如果本身就面临性能问题，可以直接测试一下看看性能有没有提高。除此之外可以通过下列命令检查一下效果：</p>

<pre><code class="sh">nscd -g | grep 'hosts cache' -A 22
</code></pre>

<p>可以得到如下的信息：</p>

<pre><code class="sh">hosts cache:

            yes  cache is enabled
            yes  cache is persistent
             no  cache is shared
            211  suggested size
         216064  total data pool size
            336  used data pool size
           3600  seconds time to live for positive entries
             20  seconds time to live for negative entries
            139  cache hits on positive entries
              4  cache hits on negative entries
              8  cache misses on positive entries
              6  cache misses on negative entries
             91% cache hit rate
              2  current number of cached values
              4  maximum number of cached values
              1  maximum chain length searched
              0  number of delays on rdlock
              0  number of delays on wrlock
              0  memory allocations failed
            yes  check /etc/hosts for changes
</code></pre>

<p>其中有命中率，cache命中的次数等信息。positive是指查询成功，negative是指查询失败。</p>

<p>但是，很多情况下，可能开启nscd之后，会发现命中率一直为0。原因在于很多linux服务会采用直接访问的模式来读取缓存，
直读的模式性能更好，但因为不经过daemon，所以无法进行统计。解决办法是把<code>shared</code>的属性关闭。</p>

<p>修改<code>/etc/nscd.conf</code>，</p>

<pre><code class="sh">       enable-cache            hosts           yes
        positive-time-to-live   hosts           3600
        negative-time-to-live   hosts           20
        suggested-size          hosts           211
        check-files             hosts           yes
        persistent              hosts           yes
        shared                  hosts           no
        max-db-size             hosts           3355443
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Linux下的虚拟化技术]]></title>
    <link href="http://octopresszhangyu.herokuapp.com/blog/2013/08/17/linuxxia-de-xu-ni-hua-ji-zhu/"/>
    <updated>2013-08-17T20:59:00+08:00</updated>
    <id>http://octopresszhangyu.herokuapp.com/blog/2013/08/17/linuxxia-de-xu-ni-hua-ji-zhu</id>
    <content type="html"><![CDATA[<p>因为要在一台Linux系统中虚拟化一个系统，于是这周接触了一下Linux下虚拟机的内容。<br/>
公司以前一直用的redhat的linux，我所知的虚拟技术仅限于xen和kvm，而且根据一些反馈用的并不是很好。主要体现在资源占用过大，硬盘和内存的消耗都很大，当然公司的服务器硬件配置也有点不够平衡。</p>

<p>然而，在接触了ubuntu的虚拟化后，一下子有茅塞顿开的感觉。首先，windows下我常用的virtualbox就可以在ubuntu下使用。xen和kvm当然ubuntu也是支持的。同时这几种虚拟化技术都可以按命令行的方式使用。其次，ubuntu在安装虚拟化套件方面是十分方便的。主要原因在于它的包要比redhat的时新的多。还有一点，ubuntu比起redhat还是很小巧的。这点在虚拟化的时候显得比较重要。毕竟一台主机的时候，只会安装一个os，如果大2GB那也就只是2GB。一旦云化后，一台主机往往就要4+1个操作系统，在磁盘不富裕的情况下，os的臃肿会成比例的放大。这时候小巧的os就显示出了优势。<br/>
所以我倾向于采用ubuntu server作为云化的主力os。至少应该是虚机的主体。</p>

<p>接下来就是选择何种东西虚拟化了。主要下面3种：</p>

<ul>
<li>xen: 国内用的最多，因为历史最长，也有商业化的支持</li>
<li>kvm：半虚拟化技术，做在kernel里面，redhat和ubuntu大力支持</li>
<li>virtualbox: 是的，其实这可能是最好用的linux虚机软件</li>
</ul>


<p>性能方面，根据<a href="http://www.liangsuilong.info/?p=675">kvm vs Virtualbox</a> 和 <a href="http://server.zol.com.cn/257/2575328.html">Ubuntu11.10 Xen Kvm Virtualbox比拼</a>，总体而言kvm的性能最佳，大文件比virtualbox有所不如。而且从未来规划来看，redhat和ubuntu普遍倾向于支持kvm。但是，virtualbox有两大优势：1.使用是图形化的界面，很简单； 2.与kvm和xen都专注于cpu性能不同，virtualbox的图形化性能要比另外两个都强许多。不过vbox实际上是基于qemu改进的，kvm-qemu也是一个组合。所以，普通用用的情况下可以直接上virtualbox。</p>

<p>顺手又了解了一些vnc的内容。vnc其实只是一个传送图形界面的协议，经典的client和server端模型，但具体的GUI还需要由其他东西实现。可选的有<code>xfce4</code>和<code>ubuntu-desktop</code>，前者小巧不少大约200MB，后者如果是server上安装则要1GB以上了。总体而言不如ssh方便，占用资源也多不少，不过在必须使用图形化界面的时候，都还是不错的选择。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[CentOS5 Ruby2.0 and Rails4.0]]></title>
    <link href="http://octopresszhangyu.herokuapp.com/blog/2013/07/12/centos5-ruby2-dot-0-and-rails4-dot-0/"/>
    <updated>2013-07-12T12:55:00+08:00</updated>
    <id>http://octopresszhangyu.herokuapp.com/blog/2013/07/12/centos5-ruby2-dot-0-and-rails4-dot-0</id>
    <content type="html"><![CDATA[<p>升级了一个很小的rails应用到4.0，原先使用的版本ree1.8.7 + rails3.1 。因为应用很小，也没使用很多插件，所以git checkout一个branch之后，用rails new生成新的目录完全替换掉老的文件。然后用<code>git diff</code>对一个一个目录或者文件进行比对与合并。整个过程下来，发现rails默认生成的文件，3.1和4.0的区别并非很大。</p>

<p>这个过程也就是只有几个小时，半天不到的时间就让应用可以跑起来了。本地测试了一下，写入数据库、查询、写入文件等都没什么问题。随后自然就是部署。</p>

<p>但此时，问题来了. 本地开发的环境是这样的：</p>

<ul>
<li>Ubuntu 12.04</li>
<li>gcc version 4.6.3 (Ubuntu/Linaro 4.6.3-1ubuntu5)</li>
<li>ruby 2.0.0p195 (2013-05-14 revision 40734) [x86_64-linux]</li>
<li>rails 4</li>
</ul>


<p>而部署环境是这样的：</p>

<ul>
<li>CentOS release 5.4</li>
<li>gcc 版本 4.1.2 20080704</li>
<li>ruby 2.0.0p247 (2013-06-27) [i686-linux]</li>
<li>rails 4</li>
</ul>


<p>在本地运作良好的rails4应用，部署到生产环境后发生下面两个异常：</p>

<p>1 <code>rake db:migrate</code>每次只能成功一个migrate，但经反复执行后，能够migrate完整
<code>
SQLite3::SQLException: SQL logic error or missing database: INSERT INTO "schema_migrations" ("version") VALUES (?)
</code>
2 开发环境下正常，生产环境下会报告<code>nil?</code>在某个object上不存在。</p>

<p>为此，则折腾掉了很多的业余时间。</p>

<p>怀疑过是Gem的版本问题，怀疑过是ruby版本的问题,也怀疑过是ruby编译问题。</p>

<p>针对第一种可能，重新生成了一个rails应用，执行的结果照旧。<br/>
为了第二种可能，使用了ruby2.0.0p195，异常依旧；换成ruby1.9.3，但发现有个atom的gem都不能bundle上去。<br/>
随后，针对第三个可能，不惜<strong>升级的了CentOS到5.9</strong>(<code>yum upgrade</code>),为此下载了600MB的更新包。 <br/>
又重新编译安装了ruby2.0.0p0, 哦?!，世界太平了。虽然第一个异常依然，但第二个异常没了，应用是能够跑的起来了。
之后，再重新编译另外两个ruby版本就都可以正常工作了（第一个问题还是在的）。</p>

<p>由此可见，这是一个和gcc版本、os版本及ruby版本都相关的bug。</p>

<h3>教训</h3>

<p>新版的ruby或Rails同CentOS接触的并不好，如果要少麻烦的话，APP的OS还是应该选择Ubuntu。从生态的情况来看，这个责任只能怨CentOS支持的内核、gcc、lib库都过于老旧了。<br/>
之所以这么说，不单单是因为在ruby上碰到这种要比在ubuntu上麻烦的多的情形。从七周七语言的情况看，CentOS即使是CentOS6，也不支持对Haskell的安装，在安装其他语言时，多多少少都会遇到编译麻烦的问题。<br/>
CentOS是基于Redhat的，以稳定（老旧）为荣。而APP则是迅速发展的，因此二者极其容易脱节。所以，APP服务器首选Ubuntu。</p>

<h3>题外话</h3>

<p>从对OS的依赖少这点来看，Java确实做了件很漂亮的事情。一个编译好的jdk文件，可以在几乎全部的主流linux下解压，然后即可使用。部署时也不用担心底层OS是什么实现。所以，如果是Java的应用，继续用CentOS也是不会有很多坑的。</p>
]]></content>
  </entry>
  
</feed>
