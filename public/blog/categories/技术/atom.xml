<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[分类: 技术 | Hegel2011的博客]]></title>
  <link href="http://octopresszhangyu.herokuapp.com/blog/categories/技术/atom.xml" rel="self"/>
  <link href="http://octopresszhangyu.herokuapp.com/"/>
  <updated>2019-12-29T11:25:00+08:00</updated>
  <id>http://octopresszhangyu.herokuapp.com/</id>
  <author>
    <name><![CDATA[Hegel 2011]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Manage It]]></title>
    <link href="http://octopresszhangyu.herokuapp.com/blog/2019/12/01/manage-it/"/>
    <updated>2019-12-01T10:49:00+08:00</updated>
    <id>http://octopresszhangyu.herokuapp.com/blog/2019/12/01/manage-it</id>
    <content type="html"><![CDATA[<p>项目的驱动因素、约束以及活动余地。少问“为什么”，少问“怎么做”</p>

<p>镀金类需求和核心需求。</p>

<h3>项目章程模板：</h3>

<ul>
<li>远景：项目的意义，远景式的话语</li>
<li>需求：项目的驱动因素</li>
<li>目标：解决技术债务、添加更多的自动化测试、设计冒烟测试，这些都可能是项目目标</li>
<li>成功标准：包括功能1、2、3，提升产品性能，测出相关数值，项目经理要确保成功标准中不会包含非项目人员才能完成的任务，比如卖出50000套软件，要确保成功标准在项目经理的掌控之中。</li>
<li>ROI估算：投资回报率</li>
</ul>


<p>质量在产品的不同市场应用阶段，可以的需求有很大区别。到大众市场之间有着摩尔鸿沟。越进入成熟阶段，对质量的要求更高，即缺陷的容忍度越低。</p>

<h3>开发项目规划模板</h3>

<ul>
<li>产品意图：（类似远景，但远景可能不够用），与远景大方向一致，但又不安全相同</li>
<li>历史记录：知道的越少，越容易惊讶</li>
<li>发布条件</li>
<li>目标：同章程中的目标，但章程编写时可能目标还不明确</li>
<li>项目组织：明确团队在项目中的职责分配，指明如何使用周期以及哪些关键实践</li>
<li>日程总览：让我的领导知道项目的实际进展</li>
<li>人员配备（人员曲线）</li>
<li>建议日程</li>
<li>风险列表：识别和管理风险</li>
</ul>


<h3>制定发布计划</h3>

<p>通过制定发布计划，和各个职能部门的人共同商讨“成功”的含义，可以意识到自己不仅只是完成分内的工作，也为项目经理指明了成功的方向。</p>

<ol>
<li>确定当前项目最重要的因素：时间、功能、低缺陷率</li>
<li>草拟发布条件，加入测试经理等人</li>
<li>SMART原则，确定的、可测量的、可达成的、相关的、可跟踪的指标和条件</li>
<li>发布条件上达成多方共识：揭示众人对于产品和项目的假定和忧虑的过程</li>
</ol>


<p>很多软件开发人员都是很乐观的。在学校时接受的训练就是要让他们变得乐观，因为那
时每个项目都会在一个学期内完成（还要花费许多个通宵）。这些训练会一直停留在他们
脑海中，直到学会估算小的功能，并接收对于估算的反馈</p>

<p>如果项目经理必须遵循某个截止日期，那就干脆什么都别估算了。把所有的功能进行排
序，并按优先级进行开发。这种状况下，我强力推荐使用敏捷生命周期，这样就可以快
速地实现功能，并马上得到反馈</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[凤凰项目读后记]]></title>
    <link href="http://octopresszhangyu.herokuapp.com/blog/2019/11/05/feng-huang-xiang-mu-du-hou-ji/"/>
    <updated>2019-11-05T12:16:00+08:00</updated>
    <id>http://octopresszhangyu.herokuapp.com/blog/2019/11/05/feng-huang-xiang-mu-du-hou-ji</id>
    <content type="html"><![CDATA[<p>在人邮图灵1024半价优惠日买的一本书，主要以小说的形式，描述了一个资深运维经理比尔接手内部的凤凰项目部署，并在世外高人埃瑞克的指导下，如何通过看板、精益、敏捷能方法打通任督二脉，满足业务部门的<strong>真实</strong>需求，从而使整个系统（集团公司）业绩达标的故事。形式相当引人入胜，也很有收获，关键是提供了和人沟通的场景和范本。</p>

<p>上手，讲述了主角被选中的原因：他过去几年组建的大型机团队，是最有组织、最值得信赖的团队，比尔本人也是一个可靠、务实，而且愿意表达真实想法的人。不过，作为运维主管，比尔对开发的印象不是很好，比如开发人员经常粗心大意弄坏东西，然后就丢给运维了；而开发人员和信息安全部门的人联手，更容易给运维添乱。而主人公的冷静坦率的做法，甚至让他在第一部分丢了工作。</p>

<p>然而，这个公司其实并不真正重视IT。比如在总体评估指标中，一个IT经理都没资格参与，即IT引起的运营风险并未被重视，也未得到管理，然而IT风险实质上是业务风险。</p>

<p>下面是一些主要的知识点：</p>

<p>倒逼项目 == 日期驱动项目，开发用完了全部的时间。这种玩法，基本后续就是起飞。大家即使都是为了公司好，关键仍然在于有限的时间里能够做什么。对工作需求、优先等级、工作进度、可用资源都了如指掌。真正令人沮丧的是无法把精力集中在对公司最为重要的事情上。</p>

<p>约束点=瓶颈，在瓶颈之外的任何地方做出改进都是假象。作为IT部副总裁，你的工作要确保形成一条迅速、可预测、持续不断的计划内工作流，从而交付价值，同时尽可能降低计划外工作的影响和破坏。</p>

<p>三步工作法：第一步，帮助我们理解在工作从开发部向运维部移动时建立快速工作流，第二步缩短及放大整个系统的反馈回路，第三步形成勇于探索且吸取教训的先进文化。（勇于冒险以及从失败中汲取教训的价值观，并强调通过反复实践以致炉火纯青的必要性）</p>

<p>四种主要的工作类型：<br/>
1. 业务项目<br/>
2. 内部IT项目<br/>
3. 变更类工作<br/>
4. 计划外（救火）工作，堪比反物质</p>

<p>在发射当口，还往太空飞船安装零件，可不是好兆头。</p>

<p>缺乏能力才是成事的大敌。</p>

<p>你的开发，把所有工作周期都用在了功能，而忽略了稳定性、安全性、可扩展性、可维护性、可操作性、持续性等。</p>

<p>相比向系统中投入更多的工作，将无用的工作剔出系统更为重要。重要的是结果，而非过程、管理或者我们完成了什么。</p>

<p>冷静、坦率应对愤怒。但上级还是会认为工程师们懒骨头。然而团队内需要信任感。</p>

<p>技术债务必将导致公司唯一能够完成的工作就是计划外工作。</p>

<p>工作中心：机器、人员、方法、测评</p>

<p>对安全中心负责人的评价：可以不对IT系统做过多无用功就保护公司，这才是你的胜利。这句话很适用于之前单位的BA。即要做过滤器，而不是放大器，要把无用的东西拒之门外。</p>

<p>和高层打交道，告诉他们坏消息要找适当的时机、适当的场合。</p>

<p>不断降低周期时间的需求是第一工作法的内容，增强回收成本的能力，是第二工作法的内容。要向高层表明‘IT导致的业务风险’。</p>

<p>之后的devops环节，将IT和运维整合在一起，通过降低发布规模，提高发版频次，比如一天10次，来实现快速交付。然而并没有提到相应的验证开销过大怎么办？</p>

<p>未来十年，每一个称职的COO都会是从IT部门出来的。否则，只能是金玉其外的傀儡，需要依靠别人来开展工作。</p>

<p>尾声当然是美好的，比尔获得了同事和上司的信任，他们业务部门的死对头莎拉也离开了公司，他和业务部门的其他人建立了良好的合作关系。公司上下更懂IT，IT也更懂公司的业务。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kafka权威指南]]></title>
    <link href="http://octopresszhangyu.herokuapp.com/blog/2019/10/08/kafkaquan-wei-zhi-nan/"/>
    <updated>2019-10-08T11:07:00+08:00</updated>
    <id>http://octopresszhangyu.herokuapp.com/blog/2019/10/08/kafkaquan-wei-zhi-nan</id>
    <content type="html"><![CDATA[<p><code>default.replication.factor</code> : 分区的复制系数<br/>
<code>unclean.leader.election</code> :
简而言之，如果我们允许不同步的副本成为首领，那么就要承担丢失数据和出现数据不一
致的风险。如果不允许它们成为首领，那么就要接受较低的可用性，因为我们必须等待原
先的首领恢复到可用状态。
如果把 unclean.leader.election.enable 设为 true，就是允许不同步的副本成为首领（也
就是“不完全的选举”），那么我们将面临丢失消息的风险。如果把这个参数设为 false，
就要等待原先的首领重新上线，从而降低了可用性。<br/>
<code>min.insync.replicas</code> : 最少同步副本</p>

<p>上述对broker的配置以外，生产者也要注意配置好<code>acks</code>的值，以及在代码里正确处理错误。</p>

<p>ETL 表示提取—转换—加载（ Extract-Transform-Load）</p>

<p>kafka的消息是可以在客户端进行压缩的</p>

<p>事件流是可重播的，借助kafka，可以重播几个月甚至几年前的原始事件流</p>

<p>时间窗口是否对齐？</p>

<p>中间结果可以作为流写入kafka中</p>

<p>flink手工启动命令
/opt/flink-1.8.1/bin/flink run /opt/flink-1.8.1/jars/ssss-1.0.0-SNAPSHOT-TEST2-cron-4d8e317-all.jar
启动命令：/opt/flink-1.8.1/bin/flink run -d /opt/flink-1.8.1/jars/Hastur-*.jar</p>

<p>罗列任务的命令：
/opt/flink-1.8.1/bin/flink list -r | grep ssss | cut -d " " -f 4</p>

<p>停止任务的命令：
/opt/flink-1.8.1/bin/flink cancel 6b338726460bcb8fe95bad883ca9f5b2</p>

<p>taskmanager.heap.size跟着yarn的container size走
taskmanager.numberOfTaskSlots 可设置多条</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[日式高效团队开发]]></title>
    <link href="http://octopresszhangyu.herokuapp.com/blog/2019/10/05/nipon-software-team/"/>
    <updated>2019-10-05T16:48:00+08:00</updated>
    <id>http://octopresszhangyu.herokuapp.com/blog/2019/10/05/nipon-software-team</id>
    <content type="html"><![CDATA[<p>日式《高效团队开发》</p>

<p>成书于2014年上半年，作者是日本人，参与了Play这个框架的开发。在日本，做到了书中提供的版本管理、缺陷管理、CI、CD公司的生产效率高很多，原因在于两点：1. 基础设施本身的效率 2. 对优秀工程师的吸引力。</p>

<p>死亡行军状态的项目---项目岌岌可危，人员身心俱疲</p>

<p>问题1：邮件内容多 -- 引入缺陷管理<br/>
问题2：没有仿真环境  -- 搭建持续、自动地向验证环境、staging 环境以及正式环 境进行发布，并一直保持正常运行的环境<br/>
问题3：没有分支和tag管理  -- 利用分支和标签<br/>
问题4：重新制作数据库的困难  -- 数据库变更管理</p>

<p>问题5: 不运行就就无法察觉问题  -- CI<br/>
问题6：代码合并后产生了degrade  -- CI+merge<br/>
问题7：重构代码没信心  -- CI+测试代码<br/>
问题9：环境依赖</p>

<p>日本的信息技术传入比美国晚3-4年，貌似中国也差不多。</p>

<p>分布式版本控制系统的缺点：<br/>
1. 没有真正的版本号<br/>
2. 没有最新的版本<br/>
3. 过于灵活，容易混乱<br/>
4. 思维方式</p>

<p>git中，应该避免分支和标签使用相同的名字，可以大致类似但细节之处有所区别</p>

<p>如果不经过长期的回归测试就不能发布，那么发布分支和开发分支应该分开来。
反之，都使用一个master为好。</p>

<p>使用Play框架的Evolution对数据库sql进行管理。类似ror的migrate，但直接使用sql。其对比语句文件是否已执行也比较巧妙。</p>

<p>缺陷驱动开发(TiDD),即以Ticket驱动开发。怎么建立gitlab和ticket之间的关联？</p>

<p>epic采用excel管理，story介于采用excel或ticket工具之间，task采用ticket工具管理</p>

<p>因成本收益和市场变化，才使得CI流行起来。
Jenkins是事实上CI工具的标准，其前身是Oracle收购Sun之前的Hudson。它的作者是一个日本人。</p>

<p>Ant、Maven、Gradle三者的关系。Ant最灵活，Maven是基于CoC后的简化版的Ant，Gradle则介于二者之间，使用groovy来完成其中灵活的那部分。</p>

<p>CI集成的测试对象：</p>

<ul>
<li>单元测试</li>
<li>集成测试</li>
<li>用户验收测试UAT</li>
<li>回归测试</li>
</ul>


<p>.git/hooks/post-receive 用于触发向jenkins push构建请求</p>

<p>CI -> CD，直接把代码库中的代码发布到产线环境</p>

<p>A 要全部采用版本管理<br/>
B 所有的环境都要用同样的方式来构建<br/>
C 要实现发布工作的自动化，并事先进行验证<br/>
D 要反复多次进行测试</p>

<p>Chef是配置并启动虚机里面的服务的，例如nginx、apache等等</p>

<p>Infrastructure as Code</p>

<p>编配（Orchestration）：<br/>
即让多个组件一起演奏起来。主要有Capistrano、Fabric（python编写）和Jenkins几种工具。
Jenkins的buidl periodically可以从多台机器上收集日志
如果采用K8S + Docker, 那么很多玩法又将大大的不同了。</p>

<p>测试四象限：</p>

<p>```</p>

<pre><code>            业务层面
            |
            |
</code></pre>

<table>
<thead>
<tr>
<th>团队的支持------- </th>
<th> -------产品的评价    </th>
</tr>
</thead>
<tbody>
<tr>
<td>                </td>
<td></td>
</tr>
</tbody>
</table>


<pre><code>            技术层面
</code></pre>

<p>```</p>

<p>单元测试属于技术和团队方面的支持，集成测试属于业务和团队方面的支持</p>

<p>总体来讲，这本书讲的日式团队软件开发，和我目前接触的有很多类似的地方。作者提到的版本管理、CI、CD我们也做了一部分，可以提高和借鉴的地方确实不少。不过时代毕竟在前进，实现栈方面作者是不提及docker和k8s的，然而这些的存在确实可以让CI/CD更进一步。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[rancher and k8s]]></title>
    <link href="http://octopresszhangyu.herokuapp.com/blog/2019/10/04/rancher-and-k8s/"/>
    <updated>2019-10-04T16:48:00+08:00</updated>
    <id>http://octopresszhangyu.herokuapp.com/blog/2019/10/04/rancher-and-k8s</id>
    <content type="html"><![CDATA[<p>今年不期而遇而使用的技术很多，除了大数据flink，就是rancher/k8s/docker了。</p>

<p>docker的话，几年前已经有过接触，总体感觉有价值，但对java来讲，帮助就比较有限了。<br/>
k8s是对docker的编排，其实就是把docker作为基本的单元，让各种服务可以在一个抽象的层面跑起来，在服务监控、伸缩等方面都提供了很好的抽象和封装，易于运维人员操作。<br/>
而Rancher，个人理解是k8s的编排，可能没那么复杂，但站在使用的角度，就便于安装、编辑、使用k8s的UI界面，当然，除了界面，幕后还有很多组件用于搭建一套k8s，启动相应的服务。</p>

<h2>安装</h2>

<p>因为他们都是基于docker的，所以前提需要安装好docker。然而，docker又是离不开OS的，所以前提是要装好OS。至于是物理主机还是虚机，倒是无所谓的。我选择使用Ubuntu 18.04 + Docker 19.03.2的组合，官方是到18.09.x。[https://rancher.com/docs/rancher/v2.x/en/installation/requirements/]</p>

<h3>安装docker</h3>

<p>```bash</p>

<h1>1. 移除旧版本</h1>

<p>sudo apt-get remove docker docker-engine docker.io</p>

<h1>2. 更新apt包索引</h1>

<p>sudo apt update</p>

<h1>3. 安装https支持包</h1>

<p>sudo apt-get install apt-transport-https ca-certificates curl software-properties-common</p>

<h1>4. 添加Docker官方GPG key</h1>

<p>curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -</p>

<h1>5. 添加稳定版的仓库源(按架构选择)</h1>

<p>sudo add-apt-repository \
   "deb [arch=amd64] https://download.docker.com/linux/ubuntu \
   $(lsb_release -cs) \
   stable"</p>

<h1>6. 更新apt包索引</h1>

<p>sudo apt update</p>

<h1>7. 安装</h1>

<p>sudo apt-get install docker-ce
```</p>

<h3>安装rancher</h3>

<p>执行下面命令即可</p>

<p><code>
docker run -d --restart=unless-stopped -p 80:80 -p 443:443 rancher/rancher:latest
</code>
直接用docker启动rancher镜像，并映射80和443端口给rancher的UI界面。随后用浏览器即可访问到rancher服务，用admin账号登录进去后，就可以进行后续的操作。</p>

<h3>增加集群</h3>

<p>在rancher的浏览器界面里，通过<code>Add Cluster</code>即可以增加节点。<br/>
首先需要增加的是etcd和controlpanel节点，这两个属于rancher的幕后服务。<br/>
命令在浏览器里会有提示，得到命令后，到要加入集群并且已经安装好docker的虚机上执行即可。命令很简单，就一条：</p>

<p>```bash
sudo docker run -d --privileged --restart=unless-stopped --net=host -v /etc/kubernetes:/etc/kubernetes -v /var/run:/var/run rancher/rancher-agent:v2.2.8 --server https://172.17.3.186 --token gjvvrqpf4sbkl2l48zmpcdpmcbcb68fntdj44vlb2784ttgct6s6wc --ca-checksum ae6f90ddff032e2d040015f70283c2e9ed5282ebdfafe0edf11e163b540dd2a7 --etcd --controlpanel</p>

<p>sudo docker run -d --privileged --restart=unless-stopped --net=host -v /etc/kubernetes:/etc/kubernetes -v /var/run:/var/run rancher/rancher-agent:v2.2.8 --server https://172.17.3.186 --token gjvvrqpf4sbkl2l48zmpcdpmcbcb68fntdj44vlb2784ttgct6s6wc --ca-checksum ae6f90ddff032e2d040015f70283c2e9ed5282ebdfafe0edf11e163b540dd2a7 --worker
```
--etcd和--controlpanel可部署在同一台机器上。然后需要耐心等待一段时间，因为会起很多个docker的服务。</p>

<p>光有控制资源是不够的，还需要加入worker资源，这个就比较方便了，可以随时扩容。</p>

<h3>新建服务</h3>

<p>上述弄完以后，就可以发布新的服务了。</p>

<p>到新建的集群中选择<code>Deploy</code>，拉取对应的镜像，设置好pod数量，就可以拉起服务。不过，这个对应的镜像主要是公共的镜像，如果私有镜像，则需要另外进行配置。</p>

<h2>新建私有docker仓库</h2>

<p>本身构造仓库并不复杂，逻辑上只要找一台虚机启动一个docker的registry服务，然后在rancher浏览器的<code>Resources-&gt;Registries</code>中注册一下这个地址即可。<br/>
但事实上永远不会那么简单。主要原因在于docker默认是需要https才能对外提供服务，这就需要进行很多额外的配置。当然，主要是私有仓库的配置。可以参考下面几个链接：
https://rancher.com/docs/rancher/v2.x/en/k8s-in-rancher/registries/
https://docs.docker.com/registry/deploying/
https://docs.docker.com/registry/insecure/</p>

<p>最后形成的做法如下。</p>

<p>一. 选择一台虚机作为registry主机，最好不要和rancher是一台机器，这样避免争抢443端口。在这台机器上先生成一下证书：
<code>bash
mkdir certs
openssl req \
  -newkey rsa:4096 -nodes -sha256 -keyout certs/domain.key \
  -x509 -days 365 -out certs/domain.crt
</code></p>

<p>这里面要注意cn需要输入自己定义的域名，比如mydocker.co</p>

<p>二. 将生成的<code>domain.crt</code>改名为<code>ca.crt</code>, 上传或复制为其他docker主机的<code>/etc/docker/certs.d/mydocker.co/ca.crt</code>，绑定这些主机的<code>hosts</code>文件中该域名的指向到上一步的ip中，或者也可以修改域名服务器里该域名</p>

<p>三. 在第一台机器上运行docker的registry服务。
<code>bash
docker run -d \
  --restart=always \
  --name registry \
  -v "/opt/registry/certs":/certs \
  -e REGISTRY_HTTP_ADDR=0.0.0.0:443 \
  -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt \
  -e REGISTRY_HTTP_TLS_KEY=/certs/domain.key \
  -p 443:443 \
  registry:2
</code></p>

<p>上面的命令中，<code>--restart=always</code>表示每次重启docker都会重启该服务，<code>-v</code>是挂载了卷，并将生成的ca证书指给了这个服务，3个<code>-e</code>设置了容器里运行的变量值，<code>-p 443:443</code>把主机和容器的443端口对应起来，<code>registry:2</code>表示第二个版本的registry</p>

<p>四. 再次deploy服务的时候，直接在镜像地址中带出<code>mydocker.co</code>开头的镜像链接即可，比如<code>mydocker.co/my-ubuntu</code> 。</p>

<h2>k8s的dns服务变迁</h2>

<p>SkyDNS（1.2） -> KubeDNS（1.4） -> CoreDNS（1.11）</p>

<p>kubedns: 监控service资源变化，生成service名称和ip的记录，并保存在DNS中<br/>
dnsmasq: 为客户端容器提供dns服务<br/>
sidecar: 对kubedns和dnsmasq提供健康检查服务</p>

<p>CoreDNS和KubeDNS均是Go语言编写，但用一个服务替换了3个服务。</p>

<h2>ingress-nginx</h2>

<p>ingress-nginx 默认是每个node 1个容器服务，<code>1 per node</code><br/>
通过rancher配置规则可以在<code>workloads-&gt;load balancing</code>中进行。里面的域名指向实际的worker主机地址。</p>

<h2>核心组件</h2>

<p>kube-proxy进程，负责获取每个Service的Endpoints，Endpoints实现service到pod之间的关联。</p>

<p>K8S的思路是每个对象都是一个资源，每个资源都有对应的controller：<br/>
* RC Controller<br/>
* Node Controller<br/>
* ResourceQutoa Controller(cpu和memory限制的配置)<br/>
* Namespace Controller<br/>
* Service Controller &amp; Endpoints Controller</p>

<p>  Kubelet是每个Node上k8s的代理，</p>

<p>kube-proxy历经了从HA proxy -> iptables -> IPVS进化。核心诉求始终是性能驱动</p>
]]></content>
  </entry>
  
</feed>
